{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display plots inline and change default figure size\n",
    "# %matplotlib inline\n",
    "\n",
    "# Package imports\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "from pandas_datareader import data as datard\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance\n",
    "import matplotlib.ticker as mticker\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, CuDNNLSTM, Conv1D\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import argrelextrema\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "\n",
    "def find_local_extreme(data):\n",
    "    df = data.copy()\n",
    "    del df['High']\n",
    "    del df['Low']\n",
    "    del df['Open']\n",
    "    prices = df.copy()\n",
    "    prices = prices.reset_index()\n",
    "    prices.columns = ['date', 'price']\n",
    "    prices = prices['price']\n",
    "    \n",
    "    kr = KernelReg([prices.values], [prices.index.to_numpy()], var_type='c')\n",
    "    f = kr.fit([prices.index])\n",
    "\n",
    "    smooth_prices = pd.Series(data=f[0], index=df.index)\n",
    "\n",
    "    # Use smoothed prices to determine local minima and maxima\n",
    "    smooth_prices = pd.Series(data=f[0], index=prices.index)\n",
    "    smooth_local_max = argrelextrema(smooth_prices.values, np.greater , order = 5)[0]\n",
    "    smooth_local_min = argrelextrema(smooth_prices.values, np.less , order = 5)[0]\n",
    "    local_max_min = np.sort(\n",
    "        np.concatenate([smooth_local_max, smooth_local_min]))\n",
    "    smooth_extrema = smooth_prices.loc[local_max_min]\n",
    "\n",
    "    # Iterate over extrema arrays returning datetime of passed\n",
    "    # prices array. Uses idxmax and idxmin to window for local extrema.\n",
    "    price_local_max_dt = []\n",
    "    for i in smooth_local_max:\n",
    "        if (i > 1) and (i < len(prices)-1):\n",
    "            price_local_max_dt.append(prices.iloc[i-2:i+2].idxmax())\n",
    "\n",
    "    price_local_min_dt = []\n",
    "    for i in smooth_local_min:\n",
    "        if (i > 1) and (i < len(prices)-1):\n",
    "            price_local_min_dt.append(prices.iloc[i-2:i+2].idxmin())\n",
    "\n",
    "    maxima = pd.Series(prices.loc[price_local_max_dt])\n",
    "    minima = pd.Series(prices.loc[price_local_min_dt])\n",
    "    extrema = pd.concat([maxima, minima]).sort_index()\n",
    "\n",
    "    # Return series for each with bar as index\n",
    "    return extrema, prices, smooth_extrema, smooth_prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def find_patterns(extrema, max_bars=35):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        extrema: extrema as pd.series with bar number as index\n",
    "        max_bars: max bars for pattern to play out\n",
    "    Returns:\n",
    "        patterns: patterns as a defaultdict list of tuples\n",
    "        containing the start and end bar of the pattern\n",
    "    \"\"\"\n",
    "    patterns = defaultdict(list)\n",
    "\n",
    "    # Need to start at five extrema for pattern generation\n",
    "    for i in range(5, len(extrema)):\n",
    "        window = extrema.iloc[i-5:i]\n",
    "\n",
    "        # A pattern must play out within max_bars (default 35)\n",
    "        if (window.index[-1] - window.index[0]) > max_bars:\n",
    "            continue\n",
    "\n",
    "        # Using the notation from the paper to avoid mistakes\n",
    "        e1 = window.iloc[0]\n",
    "        e2 = window.iloc[1]\n",
    "        e3 = window.iloc[2]\n",
    "        e4 = window.iloc[3]\n",
    "        e5 = window.iloc[4]\n",
    "\n",
    "        rtop_g1 = np.mean([e1, e3, e5])\n",
    "        rtop_g2 = np.mean([e2, e4])\n",
    "        # Head and Shoulders\n",
    "        if (e1 > e2) and (e3 > e1) and (e3 > e5) and \\\n",
    "                (abs(e1 - e5) <= 0.03*np.mean([e1, e5])) and \\\n",
    "                (abs(e2 - e4) <= 0.03*np.mean([e1, e5])):\n",
    "            patterns['HS'].append((window.index[0], window.index[-1]))\n",
    "\n",
    "        # Inverse Head and Shoulders\n",
    "        elif (e1 < e2) and (e3 < e1) and (e3 < e5) and \\\n",
    "                (abs(e1 - e5) <= 0.03*np.mean([e1, e5])) and \\\n",
    "                (abs(e2 - e4) <= 0.03*np.mean([e1, e5])):\n",
    "            patterns['IHS'].append((window.index[0], window.index[-1]))\n",
    "\n",
    "        # Broadening Top\n",
    "        elif (e1 > e2) and (e1 < e3) and (e3 < e5) and (e2 > e4):\n",
    "            patterns['BTOP'].append((window.index[0], window.index[-1]))\n",
    "\n",
    "        # Broadening Bottom\n",
    "        elif (e1 < e2) and (e1 > e3) and (e3 > e5) and (e2 < e4):\n",
    "            patterns['BBOT'].append((window.index[0], window.index[-1]))\n",
    "\n",
    "        # Triangle Top\n",
    "        elif (e1 > e2) and (e1 > e3) and (e3 > e5) and (e2 < e4):\n",
    "            patterns['TTOP'].append((window.index[0], window.index[-1]))\n",
    "\n",
    "        # Triangle Bottom\n",
    "        elif (e1 < e2) and (e1 < e3) and (e3 < e5) and (e2 > e4):\n",
    "            patterns['TBOT'].append((window.index[0], window.index[-1]))\n",
    "\n",
    "        # Rectangle Top\n",
    "        elif (e1 > e2) and \\\n",
    "                (abs(e1-rtop_g1)/rtop_g1 < 0.0075) and \\\n",
    "                (abs(e3-rtop_g1)/rtop_g1 < 0.0075) and \\\n",
    "                (abs(e5-rtop_g1)/rtop_g1 < 0.0075) and \\\n",
    "                (abs(e2-rtop_g2)/rtop_g2 < 0.0075) and \\\n",
    "                (abs(e4-rtop_g2)/rtop_g2 < 0.0075) and \\\n",
    "                (min(e1, e3, e5) > max(e2, e4)):\n",
    "\n",
    "            patterns['RTOP'].append((window.index[0], window.index[-1]))\n",
    "\n",
    "        # Rectangle Bottom\n",
    "        elif (e1 < e2) and \\\n",
    "                (abs(e1-rtop_g1)/rtop_g1 < 0.0075) and \\\n",
    "                (abs(e3-rtop_g1)/rtop_g1 < 0.0075) and \\\n",
    "                (abs(e5-rtop_g1)/rtop_g1 < 0.0075) and \\\n",
    "                (abs(e2-rtop_g2)/rtop_g2 < 0.0075) and \\\n",
    "                (abs(e4-rtop_g2)/rtop_g2 < 0.0075) and \\\n",
    "                (max(e1, e3, e5) > min(e2, e4)):\n",
    "\n",
    "            patterns['RBOT'].append((window.index[0], window.index[-1]))\n",
    "\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_window(prices, extrema, smooth_prices, smooth_extrema, ax=None):\n",
    "    \"\"\"\n",
    "    Input: data from find_extrema\n",
    "    Output: plots window for actual and smoothed prices and extrema\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=[20,14])\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "    prices.plot(ax=ax, color='dodgerblue')\n",
    "    ax.scatter(extrema.index, extrema.values, color='red')\n",
    "    smooth_prices.plot(ax=ax, color='lightgrey')\n",
    "    ax.scatter(smooth_extrema.index, smooth_extrema.values, color='lightgrey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_window(prices, extrema, smooth_prices, smooth_extrema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the price data in to actual candlestick parameters. Each candle has 4 parameters\n",
    "# Open\tHigh Low\tClose -< conversion_array\n",
    "# Size of the body measured by pips\n",
    "# Size of the upper wicks measured by pips\n",
    "# Size of the lower wicks measured by pips\n",
    "# Type of the candle (Bullish or Bearish)(Green or Red)(0 or 1)\n",
    "\n",
    "\n",
    "#Handle\n",
    "def ohlc_to_candlestick(conversion_array):\n",
    "    candlestick_data = [0,0,0,0]\n",
    "\n",
    "    if conversion_array[3]>conversion_array[0]:\n",
    "        candle_type=1\n",
    "        wicks_up=conversion_array[1]-conversion_array[3]\n",
    "        wicks_down=conversion_array[2]-conversion_array[0]\n",
    "        body_size=conversion_array[3]-conversion_array[0]\n",
    "\n",
    "    else:\n",
    "        candle_type=0\n",
    "        wicks_up=conversion_array[1]-conversion_array[0]\n",
    "        wicks_down=conversion_array[2]-conversion_array[3]\n",
    "        body_size=conversion_array[1]-conversion_array[3]\n",
    "\n",
    "\n",
    "    if wicks_up < 0:wicks_up=wicks_up*(-1)\n",
    "    if wicks_down < 0:wicks_down=wicks_down*(-1)\n",
    "    if body_size < 0:body_size=body_size*(-1)\n",
    "    \n",
    "    candlestick_data[0]=candle_type\n",
    "    candlestick_data[1]=round(round(wicks_up,5)*10000, 4)\n",
    "    candlestick_data[2]=round(round(wicks_down,5)*10000, 4)\n",
    "    candlestick_data[3]=round(round(body_size,5)*10000, 4)\n",
    "\n",
    "    return candlestick_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(df):\n",
    "    extrema, prices, smooth_extrema, smooth_prices = find_local_extreme(stock_df)\n",
    "    patterns = find_patterns(extrema)\n",
    "    cor_arr = []\n",
    "    \n",
    "    for name, pattern_periods in patterns.items():\n",
    "        if (name=='HS' or name=='IHS' or name=='TTOP' or name=='TBOT' or name=='BTOP' or name=='BBOT') :    \n",
    "            for start, end in pattern_periods:\n",
    "                x = prices.index[start-1]\n",
    "                y = prices.index[end+1]\n",
    "                if (y-x > 0):\n",
    "                    # gap = 40 - y + x \n",
    "                    # mid = int(gap / 2)\n",
    "                    # x = x - mid\n",
    "                    # y = y + gap - mid\n",
    "                    cor_arr.append([x,y])\n",
    "    X_raw_tmp = [] #Array for actual data\n",
    "    X_tmp = [] #Array for converted data\n",
    "    Y_tmp = [] #Array for prediction based on the condition whether the mean close value of next two week is higher than current close value\n",
    "\n",
    "    for row in cor_arr:\n",
    "        tmp = []\n",
    "        mean = []\n",
    "        prediction = 0\n",
    "        X_raw_tmp.append(df.values[row[0] : row[1]]) # append raw value\n",
    "\n",
    "        for idx in range(row[0] , row[1]):\n",
    "            converted_data = ohlc_to_candlestick(df.values[idx])\n",
    "            tmp.append(converted_data) # convert data to cds \n",
    "\n",
    "        for idx in range(row[1], row[1] + 14):\n",
    "            mean.append(df.values[idx][3])\n",
    "\n",
    "        X_tmp.append(tmp)\n",
    "        \n",
    "        if (np.mean(mean) > df.values[row[1]][3]):\n",
    "            prediction = 1\n",
    "        Y_tmp.append(prediction)\n",
    "\n",
    "    X_tmp = np.asarray(X_tmp)\n",
    "    X_raw_tmp = np.asarray(X_raw_tmp)\n",
    "    Y_tmp = np.asarray(Y_tmp)\n",
    "    \n",
    "    return X_tmp, Y_tmp, X_raw_tmp\n",
    "\n",
    "# X  = np.empty((0, 40 ,4))\n",
    "# Y = np.empty((0))\n",
    "# X_raw = np.empty((0, 40 ,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "stocks = ['TSLA' , 'MSFT' , 'NFLX' , 'AAPL' , 'AMZN' , 'AVGO' , 'FB' , 'OKTA' , 'REGN' , 'KHC' , 'ADSK' , 'ANSS' , 'FISV' , 'GILD' , 'AMLX']\n",
    "#, 'MSFT' , 'AMD' , 'NFLX' , 'AVGO' , 'FB' , 'AMZN'\n",
    "# , , 'ADSK' , 'ANSS' ,  'AVGO' , 'AZN' , 'FISV' , 'GILD' , 'KHC' , 'NFLX' , 'OKTA' , 'REGN\n",
    "endDate = pd.to_datetime('today')\n",
    "\n",
    "startDate = endDate - timedelta(days = 300 * 16 )\n",
    "flag = 0\n",
    "\n",
    "for stock in stocks:\n",
    "  stock_df = yfinance.download(stock , startDate , endDate)\n",
    "  del stock_df['Volume']\n",
    "  del stock_df['Adj Close']\n",
    "  X1 , Y1 , X_raw1 = pre_processing(stock_df)\n",
    "  if (flag == 0):\n",
    "    X = X1\n",
    "    Y = Y1\n",
    "    X_raw = X_raw1\n",
    "    flag = 1\n",
    "  else:\n",
    "    X = np.concatenate((X, X1) , axis= 0)\n",
    "    Y = np.concatenate((Y, Y1) , axis= 0)\n",
    "    X_raw = np.concatenate((X_raw, X_raw1) , axis= 0)\n",
    " \n",
    "\n",
    "X = pad_sequences(X)\n",
    "X_raw = pad_sequences(X_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(515, 37, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"time_distributed_7\" (type TimeDistributed).\n\nInput 0 of layer \"conv1d_5\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 37)\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, None, 37), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-4b670de5f12b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-4b670de5f12b>\u001b[0m in \u001b[0;36mnn\u001b[1;34m(shape_1, shape_2)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mshape_1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mconv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mbatch1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[0;32m    229\u001b[0m                          \u001b[1;34m'is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                          \u001b[1;34mf'expected min_ndim={spec.min_ndim}, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"time_distributed_7\" (type TimeDistributed).\n\nInput 0 of layer \"conv1d_5\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 37)\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, None, 37), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Conv1D, TimeDistributed,Dropout,Input, Dense, BatchNormalization, GRU, Layer, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def nn(shape_1,shape_2):\n",
    "   \n",
    "    input = Input(shape= (None , shape_1 , shape_2))\n",
    "\n",
    "    conv1 = TimeDistributed(Conv1D(32 , kernel_size= 32 , strides= 1 , activation='relu'))(input)\n",
    "    batch1 = TimeDistributed(BatchNormalization())(conv1)\n",
    "\n",
    "    flat = TimeDistributed(Flatten())(batch1)\n",
    "\n",
    "    gru1 = GRU(256, activation='relu',return_sequences=True, kernel_regularizer=l2(0.01))(flat)\n",
    "    drop1 = Dropout(rate=0.4)(gru1)\n",
    "    batch1 = BatchNormalization()(drop1)\n",
    "\n",
    "    gru2 = GRU(128, activation='relu',return_sequences=True, kernel_regularizer=l2(0.01))(batch1)\n",
    "    drop2 = Dropout(rate=0.4)(gru2)\n",
    "    batch2 = BatchNormalization()(drop2)\n",
    "\n",
    "    dense = TimeDistributed(Dense(1, activation='softmax'),name = 'output')(batch2)\n",
    "    return [input], [dense]\n",
    "\n",
    "\n",
    "EPOCH_LENGTH = 30\n",
    "SAMPLE_RATE = 100\n",
    "\n",
    "input, output = nn( X.shape[1],X.shape[2])\n",
    "model = Model(inputs=input,outputs=output)\n",
    "\n",
    "optimizer = Adam(learning_rate=2*1e-4)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss={\n",
    "                  'output': 'sparse_categorical_crossentropy', },\n",
    "              metrics={\n",
    "                  'output': 'sparse_categorical_accuracy', },\n",
    "              sample_weight_mode='temporal')\n",
    "model.summary()\n",
    "\n",
    "# model.add(layers.LSTM(50 , return_sequences=True , input_shape = (None, X.shape[-1])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(layers.LSTM(100 , return_sequences=False))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(layers.Dense(units = 1,activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(515, 37, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import layers\n",
    "# from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "\n",
    "# model.add(layers.GRU(60 , return_sequences=True, input_shape = (None, X.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(layers.GRU(60 , return_sequences=False))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# model.compile(optimizer= 'rmsprop' , loss=\"mean_squared_error\" , metrics=[\"acc\"])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.5 , shuffle = False)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5 , shuffle = False)\n",
    "X_train_raw, X_val_and_test_raw = train_test_split(X_raw, test_size=0.5 , shuffle = False )\n",
    "X_val_raw, X_test_raw = train_test_split(X_val_and_test_raw, test_size=0.5 , shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    \"my_checkpoint\", save_best_only=True)\n",
    "\n",
    "# Set up early stop\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, None, 37, 4), found shape=(None, 37, 4)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c5ebe7de2bcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, None, 37, 4), found shape=(None, 37, 4)\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs= 200 , validation_data=(X_val, Y_val) , callbacks=[model_checkpoint , early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 1 - Model Loss\n",
    "#plt.subplot(331)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Chart 2 - Model Accuracy\n",
    "#plt.subplot(332)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep = List of candles seqeuence\n",
    "# Items = Candlestick\n",
    "# Features = High, Low, Open, Close parametes\n",
    "def graph_data_ohlc(dataset):\n",
    "    ax1 = plt.subplot2grid((1,1), (0,0))\n",
    "    closep=dataset[:,[3]]\n",
    "    highp=dataset[:,[1]]\n",
    "    lowp=dataset[:,[2]]\n",
    "    openp=dataset[:,[0]]\n",
    "    date = range(len(closep))\n",
    "\n",
    "    x = 0\n",
    "    y = len(date)\n",
    "    ohlc = []\n",
    "\n",
    "    while x < y:\n",
    "        append_me = date[x], openp[x], highp[x], lowp[x], closep[x]\n",
    "        ohlc.append(append_me)\n",
    "        x += 1\n",
    "\n",
    "    candlestick_ohlc(ax1, ohlc, width=0.4, colorup='#77d879', colordown='#db3f3f')\n",
    "\n",
    "    for label in ax1.xaxis.get_ticklabels():\n",
    "        label.set_rotation(45)\n",
    "        \n",
    "    ax1.xaxis.set_major_locator(mticker.MaxNLocator(10))\n",
    "    ax1.grid(True)\n",
    "\n",
    "\n",
    "    plt.xlabel('Candle')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Candlestick sample representation')\n",
    "\n",
    "    plt.subplots_adjust(left=0.09, bottom=0.20, right=0.94, top=0.90, wspace=0.2, hspace=0)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter = 0\n",
    "won = 0\n",
    "lost = 0\n",
    "bullish_counter = 0\n",
    "bearish_counter = 0\n",
    "test = model.predict(X_test)\n",
    "alpha_distance = 0.4\n",
    "\n",
    "for a in test:\n",
    "    if a > (1-alpha_distance) or a < alpha_distance :\n",
    "        if Y_test[counter] == 1:\n",
    "            print('Correct trend is Bullish')\n",
    "            bullish_counter = bullish_counter + 1\n",
    "        if Y_test[counter] == 0:\n",
    "            print('Correct trend is Bearish')\n",
    "            bearish_counter = bearish_counter + 1\n",
    "        if a > (1-alpha_distance):print('Model prediction trend is Bullish')\n",
    "        if a < alpha_distance:print('Model prediction trend is Bearish')\n",
    "\n",
    "        if (a > (1-alpha_distance) and Y_test[counter] == 1) or (a < alpha_distance and Y_test[counter] == 0):\n",
    "            won=won+1\n",
    "            print('WON')\n",
    "        else:\n",
    "            print('LOST')\n",
    "            lost=lost+1\n",
    "\n",
    "        d_arr = np.empty(( 0 ,4))\n",
    "\n",
    "        d_arr = np.append(d_arr , X_test_raw[counter] , axis= 0 )\n",
    "        \n",
    "        d_arr = d_arr[~np.all(d_arr == 0, axis=1)]\n",
    "\n",
    "        graph_data_ohlc(d_arr)\n",
    "\n",
    "    counter=counter+1\n",
    "    \n",
    "print('Won: ' + str(won) + ' Lost: ' + str(lost))\n",
    "print('Success rate: ' + str(round((won*100)/(won+lost),2)) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish_counter / ( bullish_counter + bearish_counter)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
